{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92a3c62-7c3f-410a-baeb-3a1ccb826382",
   "metadata": {},
   "source": [
    "# LLM Consistency and Stability Evaluation\n",
    "\n",
    "## Overview\n",
    "This notebook implements a systematic framework for evaluating the **output consistency** and **stability** of large language models (LLMs) under repeated inference with identical inputs.  \n",
    "The analysis quantifies stochastic variability in both *numerical outputs* (probabilities) and *semantic outputs* (text explanations) generated by a fine-tuned causal language model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Experimental Objective\n",
    "Given a model $ f_\\theta(x) $ that maps prompts $ x $ to probabilistic text outputs, we assess the degree to which  \n",
    "$$\n",
    "f_\\theta^{(r)}(x) \\approx f_\\theta^{(s)}(x)\n",
    "$$\n",
    "holds for independent runs $ r, s \\in \\{1, \\dots, N\\} $, where differences arise solely from random seeds and inherent sampling stochasticity.\n",
    "\n",
    "Two aspects of stability are investigated:\n",
    "\n",
    "1. **Probability Stability** ‚Äì Consistency of numeric outputs (the ‚Äúbuy‚Äù probability).  \n",
    "2. **Semantic Stability** ‚Äì Consistency of the natural-language *explanations* across runs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prompt Construction\n",
    "Each prompt is generated using random combinations of *traits* and *contexts* describing an individual's behavioral tendencies and situational factors.  \n",
    "Formally, let:\n",
    "$$\n",
    "x_i = (T_i, C_i)\n",
    "$$\n",
    "where $T_i \\subseteq \\mathcal{T}$ and $C_i \\subseteq \\mathcal{C}$ are random subsets of predefined sets of traits and contexts.\n",
    "\n",
    "The textual prompt $p_i$ is formed using a fixed template:\n",
    "$$\n",
    "p_i = \t{Template}(T_i, C_i)\n",
    "$$\n",
    "and the model output is expected in structured JSON form:\n",
    "$$\n",
    "y_i = \\{ \t{buy}: p_i^{(buy)}, \\; \t{explanation}: e_i \\}\n",
    "$$\n",
    "where $p_i^{(buy)} \\in [0,1]$ and $e_i$ is free-form text.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Experimental Design\n",
    "\n",
    "- **Model**: A locally stored LLM (e.g., `gemma-3-4b-it`).\n",
    "- **Inference settings**: Deterministic decoding (`do_sample=False`), maximum 200 tokens per generation.\n",
    "- **Repetitions**: $ N = 5 $ independent runs with distinct random seeds.\n",
    "- **Prompts**: $ M = 30 $ distinct input contexts.\n",
    "\n",
    "This yields a data tensor of outputs:\n",
    "$$\n",
    "Y = \\{ y_{r,i} = (p_{r,i}, e_{r,i}) \\mid r = 1, \\dots, N; \\, i = 1, \\dots, M \\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Probability Stability Metrics\n",
    "\n",
    "Let $ P \\in \\mathbb{R}^{N \times M} $ denote the matrix of predicted probabilities.\n",
    "\n",
    "1. **Per-prompt standard deviation**  \n",
    "   $$\n",
    "   \\sigma_i = \\sqrt{\\frac{1}{N} \\sum_{r=1}^N (p_{r,i} - \\bar{p}_i)^2}\n",
    "   $$\n",
    "   with $\\bar{p}_i = \\frac{1}{N}\\sum_{r=1}^N p_{r,i}$.\n",
    "\n",
    "   The mean standard deviation over prompts measures overall dispersion:\n",
    "   $$\n",
    "   \\bar{\\sigma} = \\frac{1}{M}\\sum_{i=1}^M \\sigma_i\n",
    "   $$\n",
    "\n",
    "2. **Mean Absolute Relative Difference (MARD)**  \n",
    "   For all run pairs $ (r,s) $,\n",
    "   $$\n",
    "   \t{MARD}_{r,s} = \\frac{1}{M} \\sum_{i=1}^M \n",
    "   \\frac{|p_{r,i} - p_{s,i}|}{\\max\\left( \\frac{p_{r,i}+p_{s,i}}{2}, \\epsilon \\right)}\n",
    "   $$\n",
    "   and the global MARD is the average across all pairs.\n",
    "\n",
    "3. **Intraclass Correlation Coefficient (ICC)**  \n",
    "   A simplified reliability index computed as:\n",
    "   $$\n",
    "   \t{ICC} = \\frac{\\mathrm{Var}(\\bar{p}_i)}{\\mathrm{Var}(\\bar{p}_i) + \\mathrm{Var}(p_{r,i} - \\bar{p}_i)}\n",
    "   $$\n",
    "   representing the ratio of between-prompt variance to total variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Semantic Stability Metrics\n",
    "\n",
    "Let $E_{r,i}$ denote the embedding of explanation $e_{r,i}$ computed via a SentenceTransformer model.  \n",
    "For each prompt $i$:\n",
    "\n",
    "$$\n",
    "S_i = \\frac{2}{N(N-1)} \\sum_{r < s} \\cos(E_{r,i}, E_{s,i})\n",
    "$$\n",
    "where $\\cos(\\cdot,\\cdot)$ denotes cosine similarity.\n",
    "\n",
    "The average semantic similarity across all prompts is:\n",
    "$$\n",
    "\\bar{S} = \\frac{1}{M}\\sum_{i=1}^M S_i\n",
    "$$\n",
    "High $\\bar{S}$ indicates consistent semantic meaning across runs.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Computation Pipeline\n",
    "\n",
    "1. **Model Loading**  \n",
    "   The tokenizer and model are loaded from a local directory with memory-optimized settings (`bfloat16`, device map = GPU).\n",
    "\n",
    "2. **Prompt Generation**  \n",
    "   Randomized sets of traits and contexts are combined to produce $M$ unique prompts.\n",
    "\n",
    "3. **Batch Generation**  \n",
    "   Prompts are encoded, generated in batches, and decoded into text outputs.\n",
    "\n",
    "4. **Parsing**  \n",
    "   JSON outputs are parsed to extract numerical probabilities and textual explanations.\n",
    "\n",
    "5. **Stability Computation**  \n",
    "   - Numerical metrics: standard deviation, MARD, ICC.  \n",
    "   - Text metrics: mean cosine similarity between explanation embeddings.\n",
    "\n",
    "6. **GPU Memory Management**  \n",
    "   After each generation cycle, CUDA memory is explicitly cleared to prevent fragmentation and OOM errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Interpretation\n",
    "\n",
    "- **Low mean standard deviation and MARD** ‚Üí high numeric stability.  \n",
    "- **High ICC (‚âà 1)** ‚Üí strong reproducibility of probabilities across runs.  \n",
    "- **High mean cosine similarity (‚âà 1)** ‚Üí stable reasoning structure in explanations.\n",
    "\n",
    "Empirically, these metrics allow quantifying whether minor stochastic variations in inference cause significant behavioral or semantic shifts in model outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Relation to Prior Research\n",
    "\n",
    "This notebook operationalizes the conceptual framework of LLM consistency and reproducibility discussed in  \n",
    "[Wang & Wang (2025), ‚ÄúAssessing Consistency and Reproducibility in the Outputs of Large Language Models‚Äù](https://arxiv.org/pdf/2503.16974?).  \n",
    "While their study uses 50 runs across multiple financial NLP tasks, this notebook implements a scaled-down, single-model version focusing on behavioral decision prompts and continuous probability outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Output Summary\n",
    "\n",
    "The notebook concludes by reporting:\n",
    "- Mean per-prompt standard deviation  \n",
    "- Mean Absolute Relative Difference (MARD)  \n",
    "- Intraclass Correlation Coefficient (ICC)  \n",
    "- Mean semantic similarity across explanations  \n",
    "\n",
    "These collectively provide a quantitative characterization of **LLM output reproducibility** in probabilistic decision-making tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3faee166-a4b2-47e9-8d3b-7909610b7c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Lade Modell ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2eab9a14c04d66bb04746c8f3e3e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generiere Batches:   0%|          | 0/8 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "üîÑ Generiere Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [04:45<00:00, 35.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Run 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generiere Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [04:44<00:00, 35.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Run 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generiere Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [04:44<00:00, 35.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Run 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generiere Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [04:44<00:00, 35.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Run 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generiere Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [04:44<00:00, 35.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Mittelwert der SDs √ºber Prompts: 0.0000\n",
      "üìä Mean Absolute Relative Difference (MARD): 0.00%\n",
      "üìä Vereinfachte ICC-Sch√§tzung: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c847e7c91f3e44888e8b6828d9605dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9deb65af7f4d6589e20685ac286be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b102668b574e7ab8f5827b143d5561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8b3d88bae7473b85a73543aad4a2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: fd5a58c4-7e91-483b-ae5c-ef276e96364d)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912dc99d64a74078ad6353b594c46a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a4381880814793843b237478fa5f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3039d2afcd411ba51a76eea4fe9f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad4356ecfdc422d8c7948b4be958db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9764cf8ed93a4595b59d31ce55c0822a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067bfb92cb614da68bee7db528f6b82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e9915094c94edbaabaebed28d5d227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Durchschnittliche semantische √Ñhnlichkeit: 1.0000\n",
      "\n",
      "‚úÖ Stabilit√§tspr√ºfung abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LLM Consistency & Stability Evaluation Notebook\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ============================================================\n",
    "# Cache & Speicherpfade konfigurieren\n",
    "# ============================================================\n",
    "\n",
    "CACHE_DIR = \"../models/cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = CACHE_DIR\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
    "\n",
    "# ============================================================\n",
    "# SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "MODEL_PATH = \"../models/saved_models/gemma-3-4b-it\"\n",
    "N_RUNS = 5                            # Anzahl Wiederholungen\n",
    "N_PROMPTS = 30                        # wie viele Prompts getestet werden\n",
    "BATCH_SIZE = 4                        # Batchgr√∂√üe pro Generierung\n",
    "MAX_NEW_TOKENS = 200                  # Ausgabel√§nge\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "# ============================================================\n",
    "# PROMPT GENERIERUNG (gleiche Logik wie im Trainingsdatenskript)\n",
    "# ============================================================\n",
    "\n",
    "TRAITS = [\n",
    "    \"likes sweets\",\n",
    "    \"dislikes sweets\",\n",
    "    \"health-conscious\",\n",
    "    \"lactose intolerant\",\n",
    "    \"cheap\",\n",
    "    \"spender\",\n",
    "    \"impulsive buyer\"\n",
    "]\n",
    "\n",
    "CONTEXTS = [\n",
    "    \"hungry\",\n",
    "    \"on a diet\",\n",
    "    \"ice cream truck nearby\",\n",
    "    \"hot summer day\",\n",
    "    \"cold winter day\",\n",
    "    \"ice cream is cheap today (discount)\",\n",
    "    \"after a long workout\",\n",
    "    \"after lunch\",\n",
    "    \"after work\"\n",
    "]\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Your decisions in everyday life are influenced by your personal traits and the context you find yourself in.\n",
    "\n",
    "Traits: {traits}\n",
    "Context: {context}\n",
    "\n",
    "The action of interest is whether you buy ice cream.\n",
    "Please assess how likely you are to take the action \"buys ice cream\" and provide your reasoning.\n",
    "Return JSON with:\n",
    "{{\"buy\": <float between 0 and 1>, \"explanation\": \"<text>\"}}\n",
    "\"\"\"\n",
    "\n",
    "def build_prompts(n=30):\n",
    "    prompts = []\n",
    "    for _ in range(n):\n",
    "        traits = random.sample(TRAITS, k=random.randint(1,3))\n",
    "        context = random.sample(CONTEXTS, k=random.randint(1,3))\n",
    "        prompt = PROMPT_TEMPLATE.format(traits=traits, context=context)\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "prompts = build_prompts(N_PROMPTS)\n",
    "\n",
    "# ============================================================\n",
    "# MODELL LADEN\n",
    "# ============================================================\n",
    "\n",
    "print(\"üöÄ Lade Modell ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ============================================================\n",
    "# GENERATION\n",
    "# ============================================================\n",
    "\n",
    "def generate_batch(prompts, batch_size=4, max_new_tokens=200):\n",
    "    all_outputs = []\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.pad_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"üîÑ Generiere Batches\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        chat_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": p}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for p in batch_prompts\n",
    "        ]\n",
    "        inputs = tokenizer(chat_texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        for j in range(len(batch_prompts)):\n",
    "            gen_tokens = outputs[j][inputs[\"input_ids\"].shape[-1]:]\n",
    "            answer = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "            all_outputs.append(answer)\n",
    "    return all_outputs\n",
    "\n",
    "def extract_prob_and_expl(text):\n",
    "    \"\"\"Parst den JSON-Teil.\"\"\"\n",
    "    try:\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\") + 1\n",
    "        js = json.loads(text[start:end])\n",
    "        return float(js.get(\"buy\", None)), str(js.get(\"explanation\", \"\")).strip()\n",
    "    except Exception:\n",
    "        return None, text\n",
    "\n",
    "# ============================================================\n",
    "# STABILITY EXPERIMENT\n",
    "# ============================================================\n",
    "\n",
    "def run_multiple_generations(prompts, n_runs=5, batch_size=4):\n",
    "    all_probs, all_texts = [], []\n",
    "    for r in range(n_runs):\n",
    "        print(f\"\\nüß™ Run {r+1}/{n_runs}\")\n",
    "        random.seed(r)\n",
    "        torch.manual_seed(r)\n",
    "        outputs = generate_batch(prompts, batch_size=batch_size)\n",
    "        probs, texts = [], []\n",
    "        for o in outputs:\n",
    "            p, t = extract_prob_and_expl(o)\n",
    "            probs.append(p)\n",
    "            texts.append(t)\n",
    "        all_probs.append(probs)\n",
    "        all_texts.append(texts)\n",
    "        free_gpu_memory()\n",
    "    return all_probs, all_texts\n",
    "\n",
    "prob_runs, text_runs = run_multiple_generations(prompts, N_RUNS, BATCH_SIZE)\n",
    "\n",
    "# ============================================================\n",
    "# Metriken f√ºr Wahrscheinlichkeit (kontinuierlich)\n",
    "# ============================================================\n",
    "\n",
    "def compute_prob_stability(prob_matrix):\n",
    "    arr = np.array(prob_matrix)  # shape: (n_runs, n_prompts)\n",
    "    mean_std = np.nanmean(np.nanstd(arr, axis=0))\n",
    "    mard_vals = []\n",
    "    for a, b in combinations(range(arr.shape[0]), 2):\n",
    "        rel_diff = np.abs(arr[a] - arr[b]) / (np.maximum((arr[a] + arr[b]) / 2, 1e-8))\n",
    "        mard_vals.append(np.nanmean(rel_diff))\n",
    "    mean_mard = np.nanmean(mard_vals)\n",
    "    icc = np.var(arr.mean(axis=0)) / (np.var(arr.mean(axis=0)) + np.var(arr - arr.mean(axis=0)))\n",
    "    print(f\"üìä Mittelwert der SDs √ºber Prompts: {mean_std:.4f}\")\n",
    "    print(f\"üìä Mean Absolute Relative Difference (MARD): {mean_mard*100:.2f}%\")\n",
    "    print(f\"üìä Vereinfachte ICC-Sch√§tzung: {icc:.3f}\")\n",
    "    return {\"std\": mean_std, \"mard\": mean_mard, \"icc\": icc}\n",
    "\n",
    "_ = compute_prob_stability(prob_runs)\n",
    "\n",
    "# ============================================================\n",
    "# Metriken f√ºr Erkl√§rung (Text)\n",
    "# ============================================================\n",
    "\n",
    "def compute_text_stability(text_runs):\n",
    "    emb_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    sims = []\n",
    "    for j in range(len(text_runs[0])):\n",
    "        texts = [text_runs[r][j] for r in range(len(text_runs))]\n",
    "        embs = emb_model.encode(texts, convert_to_tensor=True)\n",
    "        pair_sims = []\n",
    "        for a, b in combinations(range(len(texts)), 2):\n",
    "            sim = util.cos_sim(embs[a], embs[b]).item()\n",
    "            pair_sims.append(sim)\n",
    "        sims.append(np.mean(pair_sims))\n",
    "    avg_sim = np.mean(sims)\n",
    "    print(f\"üìä Durchschnittliche semantische √Ñhnlichkeit: {avg_sim:.4f}\")\n",
    "    return avg_sim\n",
    "\n",
    "_ = compute_text_stability(text_runs)\n",
    "\n",
    "print(\"\\n‚úÖ Stabilit√§tspr√ºfung abgeschlossen.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
