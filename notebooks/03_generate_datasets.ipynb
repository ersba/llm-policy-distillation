{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018377c1-ab8a-4e83-ab5a-a47cb92bd458",
   "metadata": {},
   "source": [
    "# LLM-Based Behavioral Data Generation\n",
    "\n",
    "## Overview\n",
    "This script generates a structured dataset of behavioral decisions using a Large Language Model (LLM).  \n",
    "It systematically samples combinations of *personal traits* and *situational contexts* and queries an LLM to estimate the likelihood of taking a specific action, the action *\"buys ice cream\"*.  \n",
    "The resulting dataset is stored as JSON Lines files.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Experimental Objective\n",
    "Let the LLM be represented as a conditional function  \n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\text{LLM output for prompt } x,\n",
    "$$  \n",
    "\n",
    "where each prompt $x$ describes a unique configuration of traits and contexts.  \n",
    "The goal is to approximate the mapping\n",
    "\n",
    "$$\n",
    "x = (\\text{traits}, \\text{context}) \\mapsto y = \\mathbb{E}[\\,p(\\text{buy} \\mid x)\\,],\n",
    "$$  \n",
    "\n",
    "where $p(\\text{buy} \\mid x) \\in [0, 1]$ is the model‚Äôs estimated probability that the individual takes the action ‚Äúbuys ice cream.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Initialization\n",
    "\n",
    "A pre-trained autoregressive model (e.g., `gemma-3-4b-it`) is loaded from a local directory using Hugging Face Transformers.  \n",
    "The model operates in inference mode (`eval()`), using `bfloat16` precision and GPU memory optimization.  \n",
    "\n",
    "Formally, the model computes token-level conditional probabilities  \n",
    "\n",
    "$$\n",
    "p_\\theta(w_t \\mid w_{<t}, x),\n",
    "$$  \n",
    "\n",
    "and the script deterministically decodes the most probable sequence using greedy decoding (`do_sample=False`).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Trait and Context Definition\n",
    "\n",
    "Two finite sets are defined:\n",
    "\n",
    "$$\n",
    "\\mathcal{T} = \\{\\text{traits such as \"likes sweets\", \"cheap\", \"impulsive buyer\", ...}\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{C} = \\{\\text{contexts such as \"hot summer day\", \"after work\", ...}\\}.\n",
    "$$\n",
    "\n",
    "Each prompt uses combinations $(T, C)$ with  \n",
    "$T \\subseteq \\mathcal{T}$, $C \\subseteq \\mathcal{C}$.\n",
    "\n",
    "To prevent semantically inconsistent combinations, logical exclusion rules are applied:\n",
    "$$\n",
    "(T, C) \\text{ is valid only if } \n",
    "T \\cap \\tau_i = \\emptyset \\ \\forall \\tau_i \\in \\text{TRAIT\\_CONFLICTS}, \\quad\n",
    "$$\n",
    "$$\n",
    "C \\cap \\kappa_i = \\emptyset \\ \\forall \\kappa_i \\in \\text{CONTEXT\\_CONFLICTS}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Prompt Template\n",
    "\n",
    "Each valid pair $(T, C)$ is inserted into a fixed natural-language template:\n",
    "\n",
    "```\n",
    "Your decisions in everyday life are influenced by your personal traits and the context you find yourself in.\n",
    "\n",
    "Traits: {traits}\n",
    "Context: {context}\n",
    "\n",
    "The action of interest is whether you buy ice cream.\n",
    "Please assess how likely you are to take the action \"buys ice cream\" and provide your reasoning.\n",
    "Return JSON with:\n",
    "{\"buy\": <float between 0 and 1>, \"explanation\": \"<text>\"}\n",
    "```\n",
    "\n",
    "Thus, for each input $x_i = (T_i, C_i)$, the model produces text $y_i$ that should contain a JSON object with a numeric field `buy`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Data Generation Process\n",
    "\n",
    "For each valid trait‚Äìcontext pair, the model generates a completion $y_i = f_\\theta(x_i)$.\n",
    "The JSON portion of the model‚Äôs output is parsed to extract the scalar value.  \n",
    "\n",
    "The data record is then stored as\n",
    "```json\n",
    "{\n",
    "  \"traits\": [...],\n",
    "  \"context\": [...],\n",
    "  \"probability_LLM\": 0.74\n",
    "}\n",
    "```\n",
    "\n",
    "The full dataset is split into:\n",
    "- 80% training examples  \n",
    "- 20% test examples  \n",
    "\n",
    "and written to:\n",
    "```\n",
    "../data/train.jsonl\n",
    "../data/test.jsonl\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Batch Generation\n",
    "\n",
    "Prompts are processed in batches of size $B$ to optimize GPU utilization.  \n",
    "Each batch $\\{x_{i_1}, \\dots, x_{i_B}\\}$ is converted into a chat-style input format and tokenized jointly.  \n",
    "Generation is performed under deterministic decoding, ensuring consistent probability extraction.\n",
    "\n",
    "Formally:\n",
    "$$\n",
    "Y_B = \\text{decode}\\big( \\text{argmax}_{w_t} p_\\theta(w_t \\mid w_{<t}, x_{i_b}) \\big)_{b=1}^B.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Postprocessing and Storage\n",
    "\n",
    "Each generated text is parsed, and valid JSON fragments are extracted.  \n",
    "If the value of `\"buy\"` is missing or non-numeric, the example is discarded.  \n",
    "The remaining dataset is shuffled and partitioned for later supervised training.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Extensions\n",
    "\n",
    "- Replace the single action ‚Äúbuys ice cream‚Äù with arbitrary decision actions.  \n",
    "- Add explanation extraction for interpretability studies.  \n",
    "- Use the generated dataset for fine-tuning smaller ‚Äústudent‚Äù models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f8a70-eca4-44ff-a0dc-ed90f10ed451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Lade Modell ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fe1b2fc315412a8c6fc322d05236e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Erzeuge Prompts ...\n",
      "Generiere 9027 Prompts aus 7 Traits und 9 Contexts.\n",
      "‚û°Ô∏è 9027 Prompts erzeugt.\n",
      "üß† Generiere Modellantworten in Batches ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generiere Batches:   0%|          | 0/1129 [00:00<?, ?batch/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "üîÑ Generiere Batches:   8%|‚ñä         | 89/1129 [51:26<10:02:49, 34.78s/batch]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SETUP\n",
    "# ============================================================\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "MODEL_PATH = \"../models/saved_models/gemma-3-4b-it\"\n",
    "\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "print(\"üöÄ Lade Modell ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ============================================================\n",
    "# TRAITS & CONTEXT DEFINITIONS\n",
    "# ============================================================\n",
    "TRAITS = [\n",
    "    \"likes sweets\",\n",
    "    \"dislikes sweets\",\n",
    "    \"health-conscious\",\n",
    "    \"lactose intolerant\",\n",
    "    \"cheap\",\n",
    "    \"spender\",\n",
    "    \"impulsive buyer\"\n",
    "]\n",
    "\n",
    "CONTEXTS = [\n",
    "    \"hungry\",\n",
    "    \"on a diet\",\n",
    "    \"ice cream truck nearby\",\n",
    "    \"hot summer day\",\n",
    "    \"cold winter day\",\n",
    "    \"ice cream is cheap today (discount)\",\n",
    "    \"after a long workout\",\n",
    "    \"after lunch\",\n",
    "    \"after work\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# CONFLICT RULES\n",
    "# ============================================================\n",
    "TRAIT_CONFLICTS = [\n",
    "    {\"likes sweets\", \"dislikes sweets\"},\n",
    "    {\"cheap\", \"spender\"},\n",
    "    {\"cheap\", \"impulsive buyer\"}\n",
    "]\n",
    "\n",
    "CONTEXT_CONFLICTS = [\n",
    "    {\"hot summer day\", \"cold winter day\"},\n",
    "    {\"hungry\", \"after lunch\"}\n",
    "]\n",
    "\n",
    "def valid_combo(combo, conflicts):\n",
    "    for conflict in conflicts:\n",
    "        if conflict.issubset(set(combo)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# PROMPT TEMPLATE\n",
    "# ============================================================\n",
    "PROMPT_TEMPLATE = \"\"\"Your decisions in everyday life are influenced by your personal traits and the context you find yourself in. \n",
    "\n",
    "In this scenario, your traits are: {traits}.  \n",
    "The current context is: {context}.  \n",
    "\n",
    "The action of interest is whether you buy ice cream. \n",
    "Your decision depends on how your traits interact with the current context. \n",
    "\n",
    "Please assess how likely you are to take the action \"buys ice cream\" in this context considering factors like price, health concerns, and social influences.  \n",
    "\n",
    "Please share your decision in a JSON format.  \n",
    "The format should have one key:  \n",
    "- 'buy' (a value between 0 and 1 with intervals of 0.02, indicating the willingness or propensity to buy ice cream).\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# DATA GENERATION\n",
    "# ============================================================\n",
    "def build_prompts():\n",
    "    prompts = []\n",
    "    meta = []\n",
    "    for t_count in range(1, 7):\n",
    "        for c_count in range(1, 7):\n",
    "            trait_combos = [\n",
    "                t for t in combinations(TRAITS, t_count)\n",
    "                if valid_combo(t, TRAIT_CONFLICTS)\n",
    "            ]\n",
    "            context_combos = [\n",
    "                c for c in combinations(CONTEXTS, c_count)\n",
    "                if valid_combo(c, CONTEXT_CONFLICTS)\n",
    "            ]\n",
    "\n",
    "            sampled_traits = random.sample(trait_combos, k=min(30, len(trait_combos)))\n",
    "            sampled_contexts = random.sample(context_combos, k=min(30, len(context_combos)))\n",
    "\n",
    "            for traits in sampled_traits:\n",
    "                for context in sampled_contexts:\n",
    "                    prompt = PROMPT_TEMPLATE.format(\n",
    "                        traits=json.dumps(traits),\n",
    "                        context=json.dumps(context)\n",
    "                    )\n",
    "                    prompts.append(prompt)\n",
    "                    meta.append((traits, context))\n",
    "    print(f\"Generiere {len(prompts)} Prompts aus {len(TRAITS)} Traits und {len(CONTEXTS)} Contexts.\")\n",
    "    return prompts, meta\n",
    "\n",
    "\n",
    "\n",
    "def generate_batch(prompts, batch_size=8, max_new_tokens=200):\n",
    "    \"\"\"Erzeugt Antworten in Batches mit demselben Chat-Template wie im Chat-Skript.\"\"\"\n",
    "    all_outputs = []\n",
    "\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.pad_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"üîÑ Generiere Batches\", unit=\"batch\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "\n",
    "        # Verwende dasselbe Chatformat wie im funktionierenden Skript\n",
    "        chat_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": p}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for p in batch_prompts\n",
    "        ]\n",
    "\n",
    "        # Nur beim ersten Batch den Prompt anzeigen\n",
    "        # if i == 0:\n",
    "#             print(\"\\nüßæ Beispiel-Prompt an das Modell:\")\n",
    "#             print(\"=\" * 80)\n",
    "#             print(chat_texts[0])\n",
    "#             print(\"=\" * 80)\n",
    "\n",
    "        inputs = tokenizer(chat_texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,      # deterministisch f√ºr stabile Werte\n",
    "                #temperature=0.0,\n",
    "                #top_p=1.0,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        # Ausgabe wie im Chat-Skript decodieren\n",
    "        for j in range(len(batch_prompts)):\n",
    "            gen_tokens = outputs[j][inputs[\"input_ids\"].shape[-1]:]\n",
    "            answer = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "            # Falls das Modell ein Pr√§fix \"model\" ausgibt ‚Üí entfernen\n",
    "            if answer.lower().startswith(\"model\"):\n",
    "                answer = answer[len(\"model\"):].lstrip(\": -\\n\")\n",
    "\n",
    "            all_outputs.append(answer)\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_prob(text):\n",
    "    \"\"\"Parst den JSON-Anteil und extrahiert den 'buy'-Wert.\"\"\"\n",
    "    try:\n",
    "        json_start = text.find(\"{\")\n",
    "        json_end = text.rfind(\"}\") + 1\n",
    "        data = json.loads(text[json_start:json_end])\n",
    "        prob = float(data.get(\"buy\", None))\n",
    "        if 0 <= prob <= 1:\n",
    "            return round(prob, 2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)\n",
    "    print(\"üìã Erzeuge Prompts ...\")\n",
    "    prompts, meta = build_prompts()\n",
    "    print(f\"‚û°Ô∏è {len(prompts)} Prompts erzeugt.\")\n",
    "\n",
    "    print(\"üß† Generiere Modellantworten in Batches ...\")\n",
    "\n",
    "    # Nur die ersten 8 Prompts nehmen (1 Batch)\n",
    "    test_prompts = prompts[:8]\n",
    "    test_meta = meta[:8]\n",
    "    \n",
    "    # Einen Batch generieren\n",
    "    # results = generate_batch(test_prompts, batch_size=8)\n",
    "#     for i, (traits, context) in enumerate(test_meta):\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(f\"üß© Beispiel {i+1}\")\n",
    "#         print(f\"Traits:  {traits}\")\n",
    "#         print(f\"Context: {context}\")\n",
    "#         print(f\"üîπ Roh-Output des Modells:\")\n",
    "#         print(results[i])\n",
    "#         print(\"=\"*80)\n",
    "    results = generate_batch(prompts, batch_size=8)\n",
    "\n",
    "    DATA = []\n",
    "    for (traits, context), text in zip(meta, results):\n",
    "        prob = extract_prob(text)\n",
    "        if prob is not None:\n",
    "            DATA.append({\n",
    "                \"traits\": list(traits),\n",
    "                \"context\": list(context),\n",
    "                \"probability_LLM\": prob\n",
    "            })\n",
    "            print(f\"‚úÖ {traits} + {context} ‚Üí {prob:.2f}\")\n",
    "\n",
    "    random.shuffle(DATA)\n",
    "    split = int(0.8 * len(DATA))\n",
    "    train_data = DATA[:split]\n",
    "    test_data = DATA[split:]\n",
    "\n",
    "    # os.makedirs(\"generated_data\", exist_ok=True)\n",
    "    with open(\"../data/train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in train_data:\n",
    "            f.write(json.dumps(d) + \"\\n\")\n",
    "    with open(\"../data/test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in test_data:\n",
    "            f.write(json.dumps(d) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nüìä Fertig! {len(train_data)} Trainings- und {len(test_data)} Testbeispiele gespeichert.\")\n",
    "    free_gpu_memory()\n",
    "    print(\"üßπ GPU-Speicher bereinigt. Ende.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb05272-eec4-47ac-8e72-04b64429fc10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
