{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc9d264-0e04-4d09-9baf-4d16cb9b5034",
   "metadata": {},
   "source": [
    "# Training Different Architectures on the Ice Cream Dataset\n",
    "\n",
    "This notebook compares several neural network architectures trained on the same small, structured dataset describing traits and contextual factors that influence whether a person might buy ice cream.  \n",
    "The target is a scalar value between 0 and 1 (a probability estimated by a language model).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Representation\n",
    "\n",
    "Each data example consists of:\n",
    "\n",
    "- a list of **traits** (e.g. \"likes sweets\", \"health-conscious\"),\n",
    "- a list of **contextual factors** (e.g. \"hot summer day\", \"after lunch\"),\n",
    "- and a scalar **target probability** $y \\in [0, 1]$.\n",
    "\n",
    "All textual tokens are mapped to integer IDs using a vocabulary:\n",
    "$$\n",
    "V = \\{t_1, t_2, \\ldots, t_{|V|}\\}\n",
    "$$\n",
    "\n",
    "For a given example with tokens $(t_{i_1}, \\ldots, t_{i_L})$, the corresponding tensor of token IDs is:\n",
    "$$\n",
    "x = [i_1, i_2, \\ldots, i_L].\n",
    "$$\n",
    "\n",
    "The dataset is stored in JSONL format and loaded into PyTorch via a custom `Dataset` and `DataLoader`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Token Embedding\n",
    "\n",
    "Each token ID $i \\in \\{1, \\ldots, |V|\\}$ is mapped to a dense vector via an embedding matrix:\n",
    "$$\n",
    "E \\in \\mathbb{R}^{|V| \\times d}\n",
    "$$\n",
    "\n",
    "The embedded representation of a sequence is:\n",
    "$$\n",
    "X = [E_{i_1}, E_{i_2}, \\ldots, E_{i_L}] \\in \\mathbb{R}^{L \\times d}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Architectures\n",
    "\n",
    "### (a) Logistic Regression\n",
    "\n",
    "This model ignores token order and computes a mean-pooled embedding:\n",
    "$$\n",
    "h = \\frac{1}{L} \\sum_{j=1}^{L} E_{i_j}.\n",
    "$$\n",
    "\n",
    "A single linear transformation followed by a sigmoid produces the output probability:\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^\\top h + b)\n",
    "$$\n",
    "\n",
    "This is equivalent to a logistic regression over the averaged embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Feedforward MLP\n",
    "\n",
    "The Feedforward MLP extends the logistic regression by passing the pooled vector through multiple nonlinear layers:\n",
    "$$\n",
    "h_0 = \\frac{1}{L} \\sum_{j=1}^{L} E_{i_j}\n",
    "$$\n",
    "\n",
    "Then, for $k = 1, \\ldots, K$ hidden layers:\n",
    "$$\n",
    "h_k = \\mathrm{ReLU}(W_k h_{k-1} + b_k)\n",
    "$$\n",
    "\n",
    "Finally, the output probability is:\n",
    "$$\n",
    "\\hat{y} = \\sigma(W_{\\text{out}} h_K + b_{\\text{out}})\n",
    "$$\n",
    "\n",
    "The model can have 2–4 layers with decreasing hidden dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Encoder-only Transformer\n",
    "\n",
    "This architecture uses the Transformer encoder to model interactions between tokens.  \n",
    "Each encoder layer applies **self-attention** and a **feedforward network**:\n",
    "\n",
    "1. **Self-attention:**\n",
    "   $$\n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "   $$\n",
    "   where $Q = XW_Q$, $K = XW_K$, and $V = XW_V$.\n",
    "\n",
    "2. **Feedforward sublayer:**\n",
    "   $$\n",
    "   \\mathrm{FFN}(x) = \\mathrm{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "   $$\n",
    "\n",
    "The encoder processes all tokens in parallel (order is not explicitly encoded since positional embeddings are omitted).  \n",
    "Mean pooling aggregates token representations into a single vector, followed by a regression head with a sigmoid activation:\n",
    "$$\n",
    "\\hat{y} = \\sigma(W_{\\text{out}} \\, \\frac{1}{L}\\sum_j h_j + b_{\\text{out}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (d) Decoder-only Transformer\n",
    "\n",
    "This model is similar to GPT-style architectures.  \n",
    "It uses **masked self-attention**, ensuring that each token can only attend to previous positions:\n",
    "$$\n",
    "M_{ij} =\n",
    "\\begin{cases}\n",
    "0, & i \\ge j \\\\\n",
    "-\\infty, & i < j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The causal attention mask enforces autoregressive processing.  \n",
    "The rest of the computation is the same as in the encoder, followed by mean pooling and a regression head.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training Objective\n",
    "\n",
    "All models predict a continuous scalar $\\hat{y} \\in [0,1]$.  \n",
    "The loss function is the **mean squared error** (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{n=1}^{N} (\\hat{y}_n - y_n)^2\n",
    "$$\n",
    "\n",
    "For probabilistic interpretation, **binary cross-entropy** (BCE) can also be used:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{N} \\sum_{n=1}^{N} [y_n \\log(\\hat{y}_n) + (1 - y_n)\\log(1 - \\hat{y}_n)]\n",
    "$$\n",
    "\n",
    "Optimization uses the AdamW optimizer with learning rates around $10^{-4}$ to $10^{-3}$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training Procedure\n",
    "\n",
    "Each model is trained for a fixed number of epochs (e.g., 10) using batched gradient descent:\n",
    "\n",
    "1. Forward pass: compute $\\hat{y}$  \n",
    "2. Compute loss $\\mathcal{L}$  \n",
    "3. Backward pass: $\\nabla_\\theta \\mathcal{L}$  \n",
    "4. Update weights with AdamW\n",
    "\n",
    "Training losses are recorded and plotted per epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Model Saving\n",
    "\n",
    "Each trained model’s weights are saved for later evaluation.  \n",
    "The saving function constructs a descriptive filename:\n",
    "\n",
    "$$\n",
    "\\texttt{\\{ModelName\\}\\_d\\{Dim\\}\\_lr\\{LR\\}\\_epoch\\{E\\}.pt}\n",
    "$$\n",
    "\n",
    "and stores it in the directory:\n",
    "```\n",
    "../models/saved_models/\n",
    "```\n",
    "\n",
    "This allows consistent comparison across architectures.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Comparison\n",
    "\n",
    "By examining training loss curves and saved model files, one can assess:\n",
    "\n",
    "- how simpler models (logistic regression, MLP) compare to Transformers,\n",
    "- whether attention mechanisms improve predictive performance,\n",
    "- and how model capacity affects generalization on this small dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d729e-fdf7-48ba-a263-22b07255fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Setup\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fd4af-ac0b-4f06-8301-1e5cf2da7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab and tokenization\n",
    "vocab = [\n",
    "    \"likes sweets\", \"dislikes sweets\", \"health-conscious\", \"lactose intolerant\",\n",
    "    \"cheap\", \"spender\", \"impulsive buyer\",\n",
    "    \"hungry\", \"on a diet\", \"ice cream truck nearby\", \"hot summer day\",\n",
    "    \"cold winter day\", \"ice cream is cheap today (discount)\",\n",
    "    \"after a long workout\", \"after lunch\", \"after work\"\n",
    "]\n",
    "\n",
    "token2id = {tok: i for i, tok in enumerate(vocab)}\n",
    "\n",
    "def to_token_ids(example):\n",
    "    ids = [token2id[tok] for tok in example[\"traits\"] + example[\"context\"]]\n",
    "    return torch.tensor(ids, dtype=torch.long), torch.tensor(example[\"probability_LLM\"], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3f5a0-033b-4998-8fe7-239a0b3fb857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "class IceCreamDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line.strip()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        input_ids, target = to_token_ids(example)\n",
    "        return input_ids, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b[0] for b in batch]\n",
    "    targets = torch.stack([b[1] for b in batch])\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "    padded = torch.stack([torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)]) for x in input_ids])\n",
    "    return padded, targets\n",
    "\n",
    "\n",
    "train_dataset = IceCreamDataset(\"../data/train.jsonl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879b4ff-a8ee-455f-958a-2a8452982bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Utility\n",
    "\n",
    "def train_model(model, train_loader, optimizer, loss_fn, epochs=10):\n",
    "    train_losses = []\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, targets in train_loader:\n",
    "            input_ids, targets = input_ids.to(device), targets.to(device)\n",
    "            preds = model(input_ids)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}  Loss={avg_loss:.4f}\")\n",
    "\n",
    "    plt.plot(train_losses)\n",
    "    plt.title(f\"Training Loss: {model.__class__.__name__}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3471c06-c2a7-43a9-9cbb-0f0e558f02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Utility\n",
    "def save_model(model, optimizer, epochs, save_name=None, save_dir=\"../models/saved_models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Standardmäßiger Dateiname, falls keiner übergeben wurde\n",
    "    if save_name is None:\n",
    "        save_name = (\n",
    "            f\"{model.__class__.__name__}_\"\n",
    "            f\"d{getattr(model, 'embedding', nn.Embedding(1,1)).embedding_dim if hasattr(model, 'embedding') else 'NA'}_\"\n",
    "            f\"lr{optimizer.param_groups[0]['lr']}_\"\n",
    "            f\"epoch{epochs}.pt\"\n",
    "        )\n",
    "\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"💾 Modell gespeichert unter: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c36a4e-8f97-4f7c-be7c-0004266fd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encoder only model\n",
    "\n",
    "class BasicEncoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "encoder_model = BasicEncoderTransformer(vocab_size=len(vocab))\n",
    "optimizer = optim.AdamW(encoder_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "encoder_losses = train_model(encoder_model, train_loader, optimizer, loss_fn, epochs=10)\n",
    "\n",
    "save_model(encoder_model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308dd8f-587e-43d2-8108-cfac680bff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decoder Only Model\n",
    "\n",
    "class BasicDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        seq_len = input_ids.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1).bool()\n",
    "        x = self.decoder(x, x, tgt_mask=mask)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "decoder_model = BasicDecoderTransformer(vocab_size=len(vocab))\n",
    "optimizer = optim.AdamW(decoder_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "decoder_losses = train_model(decoder_model, train_loader, optimizer, loss_fn, epochs=10)\n",
    "\n",
    "save_model(decoder_model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6116db-0062-4d6e-bb6c-1044561424e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Binary Logistic Regression with Sigmoid\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, num_classes),\n",
    "            nn.Sigmoid()  # für Wahrscheinlichkeiten\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Einfache Mittelung über alle Tokens (Bag-of-Words-artig)\n",
    "        x = self.embedding(input_ids)   # [B, L, d_model]\n",
    "        x = x.mean(dim=1)               # [B, d_model]\n",
    "        out = self.fc(x)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "logreg_model = LogisticRegressionModel(vocab_size=len(vocab))\n",
    "optimizer = optim.AdamW(logreg_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "logreg_losses = train_model(logreg_model, train_loader, optimizer, loss_fn, epochs=10)\n",
    "\n",
    "save_model(logreg_model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b2f21-fe7c-4313-abbe-49fdf2e0d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Feedforward MLP\n",
    "\n",
    "class FeedForwardMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, hidden_dims=[128, 64], num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        layers = []\n",
    "        input_dim = d_model\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "        layers.append(nn.Sigmoid())  \n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # No Order\n",
    "        x = self.embedding(input_ids)  # [B, L, d_model]\n",
    "        x = x.mean(dim=1)              # [B, d_model]\n",
    "        out = self.network(x)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "mlp_model = FeedForwardMLP(vocab_size=len(vocab), d_model=128, hidden_dims=[128, 64])\n",
    "optimizer = optim.AdamW(mlp_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "mlp_losses = train_model(mlp_model, train_loader, optimizer, loss_fn, epochs=10)\n",
    "\n",
    "save_model(mlp_model, optimizer, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
