{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead19890-9fba-41a5-9220-e3cb5985a18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58643ff0b86e4f43804286db89c6c4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modell geladen. Chat gestartet (Tippe 'exit' zum Beenden).\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Du:  Was ist die Hauptstadt Deutschlands?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma: Die Hauptstadt Deutschlands ist Berlin.\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Abgebrochen vom Nutzer]\n",
      "\n",
      "ðŸ§¹ Speicherbereinigung lÃ¤uft ...\n",
      "âœ… GPU-Speicher freigegeben.\n",
      "ðŸš€ Skript beendet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Basis-Setup\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True             # optimiert CUDNN-Algorithmen dynamisch\n",
    "\n",
    "model_path = \"../models/saved_models/gemma-3-4b-it\"\n",
    "\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"âœ… GPU-Speicher freigegeben.\")\n",
    "\n",
    "try:\n",
    "    print(\"Lade Modell ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.pad_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    print(\"âœ… Modell geladen. Chat gestartet (Tippe 'exit' zum Beenden).\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Chat-Schleife\n",
    "    # ---------------------------------------------------------------\n",
    "    while True:\n",
    "        user_input = input(\"Du: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "            print(\"Chat beendet.\")\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,    # vermeidet Wiederholungen\n",
    "                max_new_tokens=300,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "            )\n",
    "            \n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        gen_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "        answer = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Eventuelles \"model\"-PrÃ¤fix entfernen\n",
    "        if answer.lower().startswith(\"model\"):\n",
    "            answer = answer[len(\"model\"):].lstrip(\": -\\n\")\n",
    "\n",
    "        print(f\"Gemma: {answer}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[Abgebrochen vom Nutzer]\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Fehler]: {e}\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nðŸ§¹ Speicherbereinigung lÃ¤uft ...\")\n",
    "    try:\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except NameError:\n",
    "        pass\n",
    "    free_gpu_memory()\n",
    "    print(\"ðŸš€ Skript beendet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834edbdf-c881-4087-9ad9-1864390ff67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-Speicher geleert.\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "print(\"GPU-Speicher geleert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a1158-fb65-4e87-aec2-222536ed2f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
